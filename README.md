# Sparse Matrix Operations - Database Systems Project

<div align="center">

**High-Performance Sparse Matrix Operations Using Database Principles**

*External Sorting | Parallel Processing | Compressed Sparse Formats*

</div>

---

## ğŸ“‹ Overview

This project implements efficient sparse matrix operations using database system principles. It focuses on super sparse matrices (90-99.9% sparsity) with applications in Graph Neural Networks (GNNs), demonstrating significant performance improvements through CSR/CSC formats, parallel processing, and GPU acceleration.

**Key Features:**
- Sparse matrix formats: COO, CSR, CSC
- CPU and GPU benchmarking
- Dynamic graph update optimization
- External sorting for large-scale operations
- Parallel processing with Numba

**Indexing Convention:** CSV files use 1-based indexing; internal operations use 0-based indexing.

---

## ğŸš€ Quick Start

### Run All Benchmarks

```bash
# CPU: Dense vs Sparse comparison
python dense_baseline_comparison/sparsity_comparison.py

# GNN: Dynamic graph updates (CPU)
python gnn_benchmark_comparison/gnn_benchmark_dynamic.py

# GNN: Dynamic graph updates (GPU) - PRIMARY
python gnn_benchmark_comparison/gnn_benchmark_dynamic_gpu.py

# GNN: GPU vs CPU comparison
python gnn_benchmark_comparison/gnn_benchmark_gpu.py
```

---

## ğŸ“Š Benchmark Results

### ğŸ–¥ï¸ Test Hardware

<div align="center">

| Component | Specification |
|:---------:|:-------------:|
| **CPU** | AMD Ryzen 9 8940HX (16 cores) |
| **GPU** | NVIDIA RTX 5070 Ti (5888 CUDA cores, 12GB VRAM) |
| **RAM** | 32GB DDR5 |

</div>

---

### 1ï¸âƒ£ CPU: Dense vs Sparse Performance

<div align="center">

**Matrix Size: 1000Ã—1000**

| Sparsity | Non-Zeros | Sparse Time | Dense Time | **Speedup** | Memory Ratio |
|:--------:|:---------:|:-----------:|:----------:|:-----------:|:------------:|
| **90%** | 95,178 | 0.063s | 1.184s | **18.7Ã—** | 2.6Ã— |
| **99%** | 9,954 | 0.001s | 1.114s | **826Ã—** | 25Ã— |
| **99.9%** | 999 | 0.0002s | 1.209s | **7,591Ã—** | 250Ã— |

**âœ… Result:** Sparse CSRÃ—CSC dominates at all super sparse levels. Speedup increases exponentially with sparsity.

</div>

---

### 2ï¸âƒ£ GNN: Dynamic Graph Updates (GPU - Primary)

<div align="center">

**GPU Accelerated | Early Stopping: 120s**

| Graph Size | Sparsity | Edges | Full Recomp | Incremental | **Speedup** | Winner |
|:----------:|:--------:|:-----:|:-----------:|:-----------:|:-----------:|:------:|
| **500** | 90% | 25,000 | 0.022s | 0.004s | **5.0Ã—** | âœ… Incremental |
| **500** | 99% | 249 | 0.0004s | 0.0003s | **1.4Ã—** | âœ… Incremental |
| **500** | 99.9% | 249 | 0.0003s | 0.0002s | **1.2Ã—** | âœ… Incremental |
| **1000** | 90% | 100,000 | 0.002s | 0.002s | **1.4Ã—** | âœ… Incremental |
| **1000** | 99% | 999 | 0.0007s | 0.0004s | **1.8Ã—** | âœ… Incremental |
| **1000** | 99.9% | 999 | 0.0004s | 0.0003s | **1.3Ã—** | âœ… Incremental |
| **1500** | 90% | 225,000 | 0.002s | 0.0006s | **3.6Ã—** | âœ… Incremental |
| **1500** | 99% | 2,249 | 0.0006s | 0.0002s | **2.6Ã—** | âœ… Incremental |
| **1500** | 99.9% | 2,249 | 0.0006s | 0.0002s | **2.5Ã—** | âœ… Incremental |

**âœ… Result:** GPU incremental updates consistently outperform full recomputation across all sparsity levels.

</div>

---

### 3ï¸âƒ£ GNN: Dynamic Graph Updates (CPU - Alternative)

<div align="center">

**LILâ†’CSR Format Conversion | Early Stopping: 120s**

| Sparsity | New Edges | Full Recomp | Incremental | **Speedup** | Winner |
|:--------:|:---------:|:-----------:|:-----------:|:-----------:|:------:|
| **90%** | 1 | 0.083s | 0.008s | **10.7Ã—** | âœ… Incremental |
| **90%** | 2 | 0.078s | 0.006s | **12.2Ã—** | âœ… Incremental |
| **90%** | 3 | 0.087s | 0.009s | **9.2Ã—** | âœ… Incremental |
| **99%** | 1 | 0.009s | 0.003s | **3.5Ã—** | âœ… Incremental |
| **99%** | 2 | 0.010s | 0.003s | **3.6Ã—** | âœ… Incremental |
| **99%** | 3 | 0.009s | 0.002s | **3.8Ã—** | âœ… Incremental |
| **99.9%** | 1 | 0.001s | 0.003s | **0.5Ã—** | âš ï¸ Full Recomp |
| **99.9%** | 2 | 0.001s | 0.002s | **0.6Ã—** | âš ï¸ Full Recomp |
| **99.9%** | 3 | 0.001s | 0.002s | **0.5Ã—** | âš ï¸ Full Recomp |

**âœ… Result:** CPU incremental updates (LILâ†’CSR) win at 90-99% sparsity.  
**âš ï¸ Note:** At extreme sparsity (99.9%), format conversion overhead makes full recomputation faster.

</div>

---

### 4ï¸âƒ£ GNN: GPU vs CPU Comparison

<div align="center">

**Typical GNN Sparsity (~96-98%)**

| Graph | Nodes | Sparsity | Edges | CPU Sparse | GPU Dense | **Speedup** | Winner |
|:-----:|:-----:|:--------:|:-----:|:----------:|:---------:|:-----------:|:------:|
| **Small** | 500 | 96.08% | 9,799 | 0.0048s | 0.0005s | **9.8Ã—** | ğŸš€ GPU |
| **Medium** | 1,000 | 98.02% | 19,799 | 0.0087s | 0.0026s | **3.4Ã—** | ğŸš€ GPU |
| **Large** | 1,500 | 98.02% | 44,537 | 0.0298s | 0.0041s | **7.2Ã—** | ğŸš€ GPU |

**âœ… Result:** GPU wins at all graph sizes with 3-10Ã— speedup for typical GNN sparsity levels.

</div>

---

### 5ï¸âƒ£ GPU Sparsity Tests

<div align="center">

| Sparsity | GPU Time | Consistency |
|:--------:|:--------:|:-----------:|
| **90%** | 1.98ms Â± 0.99ms | âœ“ |
| **99%** | 2.04ms Â± 0.70ms | âœ“ |
| **99.9%** | 2.37ms Â± 0.48ms | âœ“ |

**âœ… Result:** GPU performance remains stable across super sparse levels (~2ms).

</div>

---

## ğŸ¯ Key Findings

### Performance Analysis

| Scenario | Recommendation | Speedup | Optimal Approach |
|:--------:|:--------------|:-------:|:-----------------|
| **Social Networks** (90-98% sparse) | GPU incremental updates | **3-10Ã—** | Matrix-based GPU operations |
| **Citation Graphs** (99% sparse) | CPU incremental updates | **3-4Ã—** | LILâ†’CSR format conversion |
| **Molecular Structures** (99.9% sparse) | CPU full recomputation | **1.5-2Ã—** | Avoid format conversion overhead |

### Dynamic Graph Updates

âœ… **GPU (Primary):** Consistent incremental advantage across all sparsity levels  
âœ… **CPU (Alternative):** Incremental wins at 90-99% sparsity  
âš ï¸ **Threshold:** At 99.9% sparsity, format conversion overhead matters

---

## ğŸ“ Project Structure

```
DB_Project_MatMul/
â”œâ”€â”€ sparse_addition.py              # Sparse matrix addition
â”œâ”€â”€ sparse_addition_parallel.py     # Parallel addition
â”œâ”€â”€ sparse_multiplication.py         # Sparse matrix multiplication
â”œâ”€â”€ sparse_multiplication_parallel.py # Parallel multiplication
â”œâ”€â”€ external_sort.py                # External sorting for large datasets
â”œâ”€â”€ matrix_formats.py               # COO, CSR, CSC conversions
â”œâ”€â”€ generate_data.py                # Test data generation
â”œâ”€â”€ dense_baseline_comparison/      # CPU sparse vs dense benchmarks
â”œâ”€â”€ gnn_benchmark_comparison/       # Graph Neural Network benchmarks
â”‚   â”œâ”€â”€ gnn_benchmark_dynamic_gpu.py  # GPU dynamic updates (PRIMARY)
â”‚   â”œâ”€â”€ gnn_benchmark_dynamic.py      # CPU dynamic updates
â”‚   â”œâ”€â”€ gnn_benchmark_gpu.py          # GPU vs CPU comparison
â”‚   â””â”€â”€ generate_graph_data.py        # Graph data generation
â””â”€â”€ google_colab_gpu/               # GPU-specific benchmarks
```

---

## ğŸ”§ Dependencies

```bash
pip install numpy scipy numba torch tqdm tabulate
```

**Requirements:**
- Python 3.9+
- NumPy 2.0+
- SciPy 1.16+
- Numba 0.62+
- PyTorch 2.6+ (CUDA 13.0+)
- TQDM, Tabulate

---

## ğŸ“– Documentation

Comprehensive documentation available in `documentation/`:

- **DATA_GENERATION.md** - Test data generation procedures
- **SPARSE_OPERATIONS.md** - Sparse matrix operation details
- **PARALLEL_CPU.md** - Parallel processing implementation
- **VERIFICATION.md** - Correctness verification methods
- **NUMBA_SCIPY_INTEGRATION.md** - Numba optimization techniques

Each benchmark folder contains detailed README with execution instructions and result interpretation.

---

## ğŸ“ Academic Context

**Database Systems Project**  
Focus: Applying database principles to sparse matrix operations

**Key Concepts:**
- External sorting for out-of-core operations
- Index structures (CSR/CSC similar to database indexes)
- Parallel query processing techniques
- GPU acceleration (hardware-aware optimization)

---

<div align="center">

**Made with â¤ï¸ for High-Performance Computing**

*For questions or contributions, please refer to the documentation folder.*

</div>
